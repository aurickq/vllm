Namespace(trace='/home/cortex/trace.jsonl', arrival_type='trace', arrival_rate=None, arrival_seed=42, input_length=None, output_length=None, max_requests=-1, backend='vllm-engine', best_of=1, use_beam_search=False, disable_tqdm=False, save_result=True, metadata=None, result_dir=None, result_filename='/home/cortex/result-0.json', otel_trace_log_filename=None, model='/data-fast/s3/ml-dev-sfc-or-dev-misc1-k8s/yak/hf_models/mistralai/Codestral-22B-v0.1', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=8192, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=8, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=False, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', scheduler_delay_factor=0.0, enable_chunked_prefill=True, speculative_model='[ngram]', num_speculative_tokens=5, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=4, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, engine_use_ray=False, disable_log_requests=True)
INFO 08-09 20:50:06 config.py:819] Chunked prefill is enabled with max_num_batched_tokens=512.
INFO 08-09 20:50:06 llm_engine.py:175] Initializing an LLM engine (v0.5.3.post1) with config: model='/data-fast/s3/ml-dev-sfc-or-dev-misc1-k8s/yak/hf_models/mistralai/Codestral-22B-v0.1', speculative_config=SpeculativeConfig(draft_model='[ngram]', num_spec_tokens=5), tokenizer='/data-fast/s3/ml-dev-sfc-or-dev-misc1-k8s/yak/hf_models/mistralai/Codestral-22B-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=/data-fast/s3/ml-dev-sfc-or-dev-misc1-k8s/yak/hf_models/mistralai/Codestral-22B-v0.1, use_v2_block_manager=True, enable_prefix_caching=False)
INFO 08-09 20:50:07 spec_decode_worker.py:153] Configuring SpecDecodeWorker with proposer=<class 'vllm.spec_decode.ngram_worker.NGramWorker'>
INFO 08-09 20:50:07 spec_decode_worker.py:167] Configuring SpecDecodeWorker with sampler=<class 'vllm.model_executor.layers.rejection_sampler.RejectionSampler'>
INFO 08-09 20:50:11 model_runner.py:719] Starting to load model /data-fast/s3/ml-dev-sfc-or-dev-misc1-k8s/yak/hf_models/mistralai/Codestral-22B-v0.1...
INFO 08-09 20:50:16 model_runner.py:731] Loading model weights took 41.4466 GB
INFO 08-09 20:50:17 gpu_executor.py:102] # GPU blocks: 8606, # CPU blocks: 1170
INFO 08-09 20:50:18 model_runner.py:1018] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 08-09 20:50:18 model_runner.py:1022] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 08-09 20:50:19 model_runner.py:1219] Graph capturing finished in 1 secs.
Starting initial single prompt test run...
proposal_token_ids_list_without_skips []
PREFILL_INDICES [0]
SPEC_INDICES []
NON_SPEC_INDICES []
METADATA_LIST [True]
NUM_SCORING_TOKENS 0
1 output []
1 5 1 0
WARNING 08-09 20:50:21 multi_step.py:57] Prompt logprob is not supported by multi step workers. (e.g., speculative decode uses multi step workers).
proposal_token_ids_list_without_skips []
PREFILL_INDICES [0]
SPEC_INDICES []
NON_SPEC_INDICES []
METADATA_LIST [True]
NUM_SCORING_TOKENS 0
1 output []
1 5 1 0
proposal_token_ids_list_without_skips []
PREFILL_INDICES [0]
SPEC_INDICES []
NON_SPEC_INDICES []
METADATA_LIST [True]
NUM_SCORING_TOKENS 0
1 output []
1 5 1 0
proposal_token_ids_list_without_skips []
PREFILL_INDICES [0]
SPEC_INDICES []
NON_SPEC_INDICES []
METADATA_LIST [True]
NUM_SCORING_TOKENS 0
1 output []
1 5 2 -1
proposal_token_ids_list_without_skips [[781, 3055, 1113, 3284, 29480]]
PREFILL_INDICES []
SPEC_INDICES [0]
NON_SPEC_INDICES []
METADATA_LIST [False, False, False, False, False, False]
NUM_SCORING_TOKENS 6
1 output []
1 5 0 1
ERROR 08-09 20:50:21 async_llm_engine.py:56] Engine background task failed
ERROR 08-09 20:50:21 async_llm_engine.py:56] Traceback (most recent call last):
ERROR 08-09 20:50:21 async_llm_engine.py:56]   File "/home/cortex/vllm-repo/vllm/engine/async_llm_engine.py", line 46, in _log_task_completion
ERROR 08-09 20:50:21 async_llm_engine.py:56]     return_value = task.result()
ERROR 08-09 20:50:21 async_llm_engine.py:56]   File "/home/cortex/vllm-repo/vllm/engine/async_llm_engine.py", line 641, in run_engine_loop
ERROR 08-09 20:50:21 async_llm_engine.py:56]     result = task.result()
ERROR 08-09 20:50:21 async_llm_engine.py:56]   File "/home/cortex/vllm-repo/vllm/engine/async_llm_engine.py", line 584, in engine_step
ERROR 08-09 20:50:21 async_llm_engine.py:56]     request_outputs = await self.engine.step_async(virtual_engine)
ERROR 08-09 20:50:21 async_llm_engine.py:56]   File "/home/cortex/vllm-repo/vllm/engine/async_llm_engine.py", line 258, in step_async
ERROR 08-09 20:50:21 async_llm_engine.py:56]     request_outputs = self._process_model_outputs(
ERROR 08-09 20:50:21 async_llm_engine.py:56]   File "/home/cortex/vllm-repo/vllm/engine/llm_engine.py", line 819, in _process_model_outputs
ERROR 08-09 20:50:21 async_llm_engine.py:56]     seq_group.update_num_computed_tokens(
ERROR 08-09 20:50:21 async_llm_engine.py:56]   File "/home/cortex/vllm-repo/vllm/sequence.py", line 571, in update_num_computed_tokens
ERROR 08-09 20:50:21 async_llm_engine.py:56]     seq.data.update_num_computed_tokens(num_new_computed_tokens)
ERROR 08-09 20:50:21 async_llm_engine.py:56]   File "/home/cortex/vllm-repo/vllm/sequence.py", line 201, in update_num_computed_tokens
ERROR 08-09 20:50:21 async_llm_engine.py:56]     assert self._num_computed_tokens <= self.get_len(), (
ERROR 08-09 20:50:21 async_llm_engine.py:56] AssertionError: (1959, 1958)
